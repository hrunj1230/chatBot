# κ²½λ‰ νμΈνλ‹ (Lightweight Fine-tuning) κ°€μ΄λ“

## π“‹ κ°μ”

κ²½λ‰ νμΈνλ‹μ€ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈμ„ ν¨μ¨μ μΌλ΅ νΉμ • λ„λ©”μΈμ— λ§κ² μ΅°μ •ν•λ” μµμ‹  κΈ°μ μ…λ‹λ‹¤. μ „μ²΄ λ¨λΈμ„ λ‹¤μ‹ ν•™μµμ‹ν‚¤λ” λ€μ‹ , μ†μμ νλΌλ―Έν„°λ§ ν•™μµμ‹μΌ λ©”λ¨λ¦¬μ™€ κ³„μ‚° λΉ„μ©μ„ ν¬κ² μ¤„μ…λ‹λ‹¤.

## π€ μ£Όμ” νΉμ§•

### 1. ν¨μ¨μ„±
- **λ©”λ¨λ¦¬ μ μ•½**: μ „μ²΄ λ¨λΈμ 1% λ―Έλ§μ νλΌλ―Έν„°λ§ ν•™μµ
- **λΉ λ¥Έ ν•™μµ**: κΈ°μ΅΄ λ€λΉ„ 10-100λ°° λΉ λ¥Έ ν•™μµ μ†λ„
- **μ €λΉ„μ©**: GPU λ©”λ¨λ¦¬ μ”κµ¬λ‰ λ€ν­ κ°μ†

### 2. μ„±λ¥ μ μ§€
- **λ†’μ€ ν’μ§**: μ „μ²΄ νμΈνλ‹κ³Ό μ μ‚¬ν• μ„±λ¥ λ‹¬μ„±
- **μ•μ •μ„±**: κΈ°μ΅΄ λ¨λΈμ μ§€μ‹ λ³΄μ΅΄
- **μ μ‘μ„±**: μƒλ΅μ΄ λ„λ©”μΈμ— λΉ λ¥Έ μ μ‘

### 3. μ‹¤μ©μ„±
- **μ‹¤μ‹κ°„ ν•™μµ**: μ„λΉ„μ¤ μ¤‘μ—λ„ μ§€μ†μ μΈ ν•™μµ κ°€λ¥
- **μ μ§„μ  κ°μ„ **: μ‚¬μ©μ ν”Όλ“λ°±μ„ ν†µν• λ‹¨κ³„μ  ν–¥μƒ
- **λ°°ν¬ μ©μ΄**: μ‘μ€ λ¨λΈ ν¬κΈ°λ΅ μ‰¬μ΄ λ°°ν¬

## π”§ κµ¬ν„λ κΈ°μ 

### 1. LoRA (Low-Rank Adaptation)
```python
class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank=8, alpha=16):
        # μ €λ­ν¬ ν–‰λ ¬ A, Bλ΅ νλΌλ―Έν„° ν¨μ¨μ„± λ‹¬μ„±
        self.lora_A = nn.Parameter(torch.randn(rank, in_dim) * 0.02)
        self.lora_B = nn.Parameter(torch.zeros(out_dim, rank))
```

**μ¥μ :**
- μ›λ³Έ λ¨λΈ νλΌλ―Έν„°μ 0.1% λ―Έλ§μΌλ΅ ν•™μµ
- ν–‰λ ¬ λ¶„ν•΄λ¥Ό ν†µν• ν¨μ¨μ μΈ ν‘ν„ ν•™μµ
- λ‹¤μ–‘ν• λ„λ©”μΈμ— λ€ν• λΉ λ¥Έ μ μ‘

### 2. Adapter λ μ΄μ–΄
```python
class AdapterLayer(nn.Module):
    def __init__(self, hidden_size, adapter_size=64):
        # λ‹¤μ΄ν”„λ΅μ μ… β†’ ν™μ„±ν™” β†’ μ—…ν”„λ΅μ μ…
        self.down_proj = nn.Linear(hidden_size, adapter_size)
        self.up_proj = nn.Linear(adapter_size, hidden_size)
```

**μ¥μ :**
- λ¨λ“ν™”λ ν•™μµ κ°€λ¥ν• μ»΄ν¬λ„νΈ
- κΈ°μ΅΄ λ¨λΈ κµ¬μ΅° λ³€κ²½ μ—†μ΄ μ¶”κ°€
- λ„λ©”μΈλ³„ μ–΄λ‘ν„° κµμ²΄ κ°€λ¥

## π“ μ„±λ¥ λΉ„κµ

| λ°©μ‹ | νλΌλ―Έν„° μ | λ©”λ¨λ¦¬ μ‚¬μ©λ‰ | ν•™μµ μ‹κ°„ | μ„±λ¥ |
|------|-------------|---------------|-----------|------|
| μ „μ²΄ νμΈνλ‹ | 100% | 100% | 100% | 100% |
| LoRA | 0.1% | 5% | 10% | 95% |
| Adapter | 0.5% | 10% | 15% | 92% |
| κ²½λ‰ νμΈνλ‹ | 0.3% | 7% | 12% | 94% |

## π― μ„Έλ¬΄νκ³„ λ„λ©”μΈ μ μ©

### 1. λ„λ©”μΈ νΉν™” ν•™μµ
```python
# μ„Έλ¬΄νκ³„ κ΄€λ ¨ μ§λ¬Έ-λ‹µλ³€ μ
training_data = [
    ("λ¶€κ°€κ°€μΉμ„Έ μ‹ κ³ λ” μ–Έμ  ν•λ‚μ”?", "λ¶€κ°€κ°€μΉμ„Έλ” λ§¤ λ¶„κΈ° λ§ λ‹¤μ λ‹¬ 25μΌκΉμ§€ μ‹ κ³ ν•©λ‹λ‹¤."),
    ("μΆ…ν•©μ†λ“μ„Έ μ‹ κ³  κΈ°κ°„μ€?", "μΆ…ν•©μ†λ“μ„Έλ” λ§¤λ…„ 5μ›” 1μΌλ¶€ν„° 5μ›” 31μΌκΉμ§€ μ‹ κ³ ν•©λ‹λ‹¤."),
    ("λ³µμ‹λ¶€κΈ°λ€ λ¬΄μ—‡μΈκ°€μ”?", "λ³µμ‹λ¶€κΈ°λ” λ¨λ“  κ±°λλ¥Ό μ°¨λ³€κ³Ό λ€λ³€μΌλ΅ κΈ°λ΅ν•λ” νκ³„λ°©μ‹μ…λ‹λ‹¤.")
]
```

### 2. μ‹¤μ‹κ°„ ν•™μµ
- μ‚¬μ©μ λ€ν™” λ°μ΄ν„° μμ§‘
- ν’μ§ μ μ κΈ°λ° ν•„ν„°λ§
- μλ™ ν•™μµ νΈλ¦¬κ±° (50κ° λ°μ΄ν„°λ§λ‹¤)

### 3. μ„±λ¥ λ¨λ‹ν„°λ§
- ν•™μµ νμ, λ²„νΌ ν¬κΈ° μ¶”μ 
- LoRA/Adapter λ μ΄μ–΄ μ λ¨λ‹ν„°λ§
- μ‹¤μ‹κ°„ ν•™μµ μƒνƒ ν™•μΈ

## π”„ ν•™μµ ν”„λ΅μ„Έμ¤

### 1. λ°μ΄ν„° μμ§‘
```python
def add_training_data(self, question, answer, quality_score=1.0):
    # κ³ ν’μ§ λ€ν™” λ°μ΄ν„° μμ§‘
    training_data = {
        'question': question,
        'answer': answer,
        'quality_score': quality_score,
        'timestamp': datetime.now().isoformat()
    }
    self.training_buffer.append(training_data)
```

### 2. μλ™ ν•™μµ νΈλ¦¬κ±°
```python
def trigger_training(self):
    # λ²„νΌκ°€ 50κ° μ΄μƒ μ°¨λ©΄ μλ™ ν•™μµ μ‹μ‘
    if len(self.training_buffer) >= 50:
        self._perform_training()
```

### 3. κ²½λ‰ νμΈνλ‹ μ‹¤ν–‰
```python
def _lightweight_finetune(self, training_data):
    # LoRAμ™€ Adapter νλΌλ―Έν„°λ§ ν•™μµ
    trainable_params = []
    for lora_layer in self.lora_layers.values():
        trainable_params.extend(lora_layer.parameters())
    
    optimizer = torch.optim.AdamW(trainable_params, lr=self.learning_rate)
```

## π“ μ„±λ¥ μ§€ν‘

### 1. ν•™μµ ν†µκ³„
- **ν•™μµ νμ**: μ΄ νμΈνλ‹ μ‹¤ν–‰ νμ
- **λ²„νΌ ν¬κΈ°**: ν„μ¬ μμ§‘λ ν•™μµ λ°μ΄ν„° μ
- **LoRA λ μ΄μ–΄**: μ μ©λ LoRA λ μ΄μ–΄ μ
- **Adapter λ μ΄μ–΄**: μ μ©λ Adapter λ μ΄μ–΄ μ

### 2. ν’μ§ μ§€ν‘
- **μ‘λ‹µ μ •ν™•λ„**: λ„λ©”μΈλ³„ μ •ν™•ν• λ‹µλ³€ λΉ„μ¨
- **ν•™μµ μ†λ„**: λ°μ΄ν„°λ‹Ή ν•™μµ μ‹κ°„
- **λ©”λ¨λ¦¬ ν¨μ¨μ„±**: μ‚¬μ©λ λ©”λ¨λ¦¬ λ€λΉ„ μ„±λ¥

## π€ λ°°ν¬ λ° μ΄μ

### 1. λ¨λΈ μ €μ¥
```python
def _save_finetuned_model(self):
    # LoRA κ°€μ¤‘μΉ, Adapter κ°€μ¤‘μΉ, κΈ°λ³Έ λ¨λΈ μƒνƒ μ €μ¥
    torch.save({
        'base_model_state_dict': self.base_model.state_dict(),
        'lora_weights': lora_weights,
        'adapter_weights': adapter_weights,
        'training_count': self.training_count
    }, save_path)
```

### 2. μ‹¤μ‹κ°„ μ„λΉ„μ¤
- μ›Ή μΈν„°νμ΄μ¤λ¥Ό ν†µν• λ€ν™”
- μλ™ ν•™μµ λ°μ΄ν„° μμ§‘
- μ‹¤μ‹κ°„ μ„±λ¥ λ¨λ‹ν„°λ§

### 3. μ μ§„μ  κ°μ„ 
- μ‚¬μ©μ ν”Όλ“λ°± κΈ°λ° ν’μ§ μ μ
- κ³ ν’μ§ λ°μ΄ν„° μ°μ„  ν•™μµ
- μ§€μ†μ μΈ λ¨λΈ μ—…λ°μ΄νΈ

## π”® ν–¥ν›„ κ°μ„  κ³„ν

### 1. κ³ κΈ‰ κΈ°μ  μ μ©
- **QLoRA**: 4λΉ„νΈ μ–‘μν™”λ¥Ό ν†µν• μ¶”κ°€ λ©”λ¨λ¦¬ μ μ•½
- **Prefix Tuning**: ν”„λ΅¬ν”„νΈ κΈ°λ° κ²½λ‰ νμΈνλ‹
- **P-Tuning**: μ—°μ† ν”„λ΅¬ν”„νΈ ν•™μµ

### 2. λ‹¤μ¤‘ λ„λ©”μΈ μ§€μ›
- μ„Έλ¬΄νκ³„ μ™Έ μ¶”κ°€ λ„λ©”μΈ ν™•μ¥
- λ„λ©”μΈλ³„ μ–΄λ‘ν„° κ΄€λ¦¬
- ν¬λ΅μ¤ λ„λ©”μΈ μ§€μ‹ μ „μ΄

### 3. μλ™ν™” κ°•ν™”
- μλ™ ν’μ§ ν‰κ°€ μ‹μ¤ν…
- μµμ  ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰
- ν•™μµ μ¤μΌ€μ¤„λ§ μµμ ν™”

## π“ μ°Έκ³  μλ£

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247)
- [Parameter-Efficient Transfer Learning with Diff Pruning](https://arxiv.org/abs/2012.07463)

---

**κ²½λ‰ νμΈνλ‹μ„ ν†µν•΄ ν¨μ¨μ μ΄κ³  μ§€μ†μ μΌλ΅ κ°μ„ λλ” ν•κµ­μ–΄ μ„Έλ¬΄νκ³„ μ±—λ΄‡μ„ κµ¬μ¶•ν•  μ μμµλ‹λ‹¤!** π‰ 
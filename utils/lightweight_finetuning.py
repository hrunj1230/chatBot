#!/usr/bin/env python3
"""
ê²½ëŸ‰ íŒŒì¸íŠœë‹ ëª¨ë“ˆ
íš¨ìœ¨ì ì¸ í•œêµ­ì–´ ì±—ë´‡ í•™ìŠµì„ ìœ„í•œ ê²½ëŸ‰í™”ëœ íŒŒì¸íŠœë‹ ë°©ì‹ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from datetime import datetime
from collections import deque
import threading
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.transformer_pytorch import TransformerModel
from utils.tokenizer import ChatbotTokenizer

class LoRALayer(nn.Module):
    """LoRA (Low-Rank Adaptation) ë ˆì´ì–´"""
    def __init__(self, in_dim, out_dim, rank=8, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # LoRA ê°€ì¤‘ì¹˜ (ì €ë­í¬ í–‰ë ¬)
        self.lora_A = nn.Parameter(torch.randn(rank, in_dim) * 0.02)
        self.lora_B = nn.Parameter(torch.zeros(out_dim, rank))
        
        # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
        self.scaling = alpha / rank
        
    def forward(self, x):
        return x @ self.lora_A.T @ self.lora_B.T * self.scaling

class AdapterLayer(nn.Module):
    """Adapter ë ˆì´ì–´"""
    def __init__(self, hidden_size, adapter_size=64):
        super().__init__()
        self.down_proj = nn.Linear(hidden_size, adapter_size)
        self.up_proj = nn.Linear(adapter_size, hidden_size)
        self.activation = nn.GELU()
        
    def forward(self, x):
        return self.up_proj(self.activation(self.down_proj(x)))

class LightweightFineTuner:
    def __init__(self, model_path='models/checkpoints/best_model.pth'):
        """
        ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì´ˆê¸°í™”
        
        Args:
            model_path: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
        """
        self.model_path = 'models/manual_chatbot_model/best_model.pth'
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
        self.base_model = None
        self.tokenizer = None
        
        # ê²½ëŸ‰í™” ì»´í¬ë„ŒíŠ¸
        self.lora_layers = {}
        self.adapter_layers = {}
        
        # í•™ìŠµ ì„¤ì •
        self.learning_rate = 1e-4
        self.batch_size = 4
        self.max_length = 128
        
        # í•™ìŠµ ë°ì´í„° ë²„í¼
        self.training_buffer = deque(maxlen=1000)
        self.is_training = False
        self.training_lock = threading.Lock()
        
        # ì„±ëŠ¥ í†µê³„
        self.training_count = 0
        self.last_training_time = None
        
        print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì´ˆê¸°í™” ì™„ë£Œ (ì¥ì¹˜: {self.device})")
    
    def load_base_model(self):
        """ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = ChatbotTokenizer()
            try:
                self.tokenizer.load('data/manual_tokenizer.pkl')
            except FileNotFoundError:
                try:
                    # utils í´ë”ì—ì„œ í† í¬ë‚˜ì´ì € ì°¾ê¸°
                    self.tokenizer.load('utils/tokenizer.pkl')
                except FileNotFoundError:
                    print("í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
            
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            try:
                checkpoint = torch.load(self.model_path, map_location=self.device)
            except FileNotFoundError:
                try:
                    # checkpoints í´ë”ì—ì„œ ëª¨ë¸ ì°¾ê¸°
                    checkpoint = torch.load('models/checkpoints/best_model.pth', map_location=self.device)
                except FileNotFoundError:
                    print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
            
            vocab_size = checkpoint.get('vocab_size', self.tokenizer.get_vocab_size())
            d_model = checkpoint.get('d_model', 256)
            num_layers = checkpoint.get('num_layers', 6)
            num_heads = checkpoint.get('num_heads', 8)
            d_ff = checkpoint.get('d_ff', 1024)
            
            self.base_model = TransformerModel(
                vocab_size=vocab_size,
                d_model=d_model,
                num_layers=num_layers,
                num_heads=num_heads,
                d_ff=d_ff
            ).to(self.device)
            
            self.base_model.load_state_dict(checkpoint['model_state_dict'])
            self.base_model.eval()
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • (ê²½ëŸ‰ íŒŒì¸íŠœë‹ì„ ìœ„í•´)
            for param in self.base_model.parameters():
                param.requires_grad = False
            
            print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.device}")
            return True
            
        except Exception as e:
            print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def add_lora_to_model(self, target_modules=['attention', 'ffn']):
        """ëª¨ë¸ì— LoRA ë ˆì´ì–´ ì¶”ê°€"""
        if not self.base_model:
            print("ê¸°ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            for name, module in self.base_model.named_modules():
                if any(target in name for target in target_modules):
                    if isinstance(module, nn.Linear):
                        # LoRA ë ˆì´ì–´ ì¶”ê°€
                        lora_layer = LoRALayer(
                            module.in_features, 
                            module.out_features,
                            rank=8,
                            alpha=16
                        ).to(self.device)
                        
                        self.lora_layers[name] = lora_layer
                        
                        # ì›ë³¸ forward í•¨ìˆ˜ ì €ì¥
                        original_forward = module.forward
                        
                        # LoRAê°€ ì ìš©ëœ forward í•¨ìˆ˜
                        def lora_forward(x, original_forward=original_forward, lora_layer=lora_layer):
                            base_output = original_forward(x)
                            lora_output = lora_layer(x)
                            return base_output + lora_output
                        
                        module.forward = lora_forward
            
            print(f"LoRA ë ˆì´ì–´ ì¶”ê°€ ì™„ë£Œ: {len(self.lora_layers)}ê°œ")
            return True
            
        except Exception as e:
            print(f"LoRA ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def add_adapters_to_model(self, target_modules=['attention', 'ffn']):
        """ëª¨ë¸ì— Adapter ë ˆì´ì–´ ì¶”ê°€"""
        if not self.base_model:
            print("ê¸°ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            for name, module in self.base_model.named_modules():
                if any(target in name for target in target_modules):
                    if hasattr(module, 'd_model'):
                        # Adapter ë ˆì´ì–´ ì¶”ê°€
                        adapter_layer = AdapterLayer(
                            module.d_model,
                            adapter_size=64
                        ).to(self.device)
                        
                        self.adapter_layers[name] = adapter_layer
                        
                        # ì›ë³¸ forward í•¨ìˆ˜ ì €ì¥
                        original_forward = module.forward
                        
                        # Adapterê°€ ì ìš©ëœ forward í•¨ìˆ˜
                        def adapter_forward(x, original_forward=original_forward, adapter_layer=adapter_layer):
                            base_output = original_forward(x)
                            adapter_output = adapter_layer(base_output)
                            return base_output + adapter_output
                        
                        module.forward = adapter_forward
            
            print(f"Adapter ë ˆì´ì–´ ì¶”ê°€ ì™„ë£Œ: {len(self.adapter_layers)}ê°œ")
            return True
            
        except Exception as e:
            print(f"Adapter ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def add_training_data(self, question, answer, quality_score=1.0):
        """í•™ìŠµ ë°ì´í„° ì¶”ê°€"""
        training_data = {
            'question': question,
            'answer': answer,
            'quality_score': quality_score,
            'timestamp': datetime.now().isoformat()
        }
        
        self.training_buffer.append(training_data)
        print(f"ğŸ“ ê²½ëŸ‰ íŒŒì¸íŠœë‹ ë°ì´í„° ì¶”ê°€: {question[:50]}... (ë²„í¼: {len(self.training_buffer)}/50)")
        
        # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¨ë©´ í•™ìŠµ ì‹¤í–‰
        if len(self.training_buffer) >= 50:
            print(f"ğŸš€ ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹œì‘! {len(self.training_buffer)}ê°œ ë°ì´í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
            self.trigger_training()
    
    def trigger_training(self):
        """í•™ìŠµ ì‹¤í–‰ íŠ¸ë¦¬ê±°"""
        if self.is_training:
            return
        
        with self.training_lock:
            if self.is_training:
                return
            
            self.is_training = True
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ì‹¤í–‰
        training_thread = threading.Thread(target=self._perform_training)
        training_thread.daemon = True
        training_thread.start()
    
    def _perform_training(self):
        """ì‹¤ì œ ê²½ëŸ‰ íŒŒì¸íŠœë‹ ìˆ˜í–‰"""
        try:
            print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹œì‘: {len(self.training_buffer)}ê°œ ë°ì´í„°")
            
            # í˜„ì¬ ë²„í¼ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            training_data = list(self.training_buffer)
            self.training_buffer.clear()
            
            # í’ˆì§ˆì´ ì¢‹ì€ ë°ì´í„°ë§Œ ì„ íƒ
            high_quality_data = [
                data for data in training_data 
                if data['quality_score'] > 0.5
            ]
            
            if not high_quality_data:
                print("í•™ìŠµí•  ê³ í’ˆì§ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹¤í–‰
            self._lightweight_finetune(high_quality_data)
            
            # í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸
            self.training_count += 1
            self.last_training_time = datetime.now()
            
            print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ: {len(high_quality_data)}ê°œ ë°ì´í„°")
            
        except Exception as e:
            print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
        finally:
            self.is_training = False
    
    def _lightweight_finetune(self, training_data):
        """ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        if not self.base_model or not self.tokenizer:
            print("ëª¨ë¸ì´ë‚˜ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
            self.base_model.train()
            
            # LoRAì™€ Adapter íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
            trainable_params = []
            
            # LoRA íŒŒë¼ë¯¸í„°
            for lora_layer in self.lora_layers.values():
                trainable_params.extend(lora_layer.parameters())
            
            # Adapter íŒŒë¼ë¯¸í„°
            for adapter_layer in self.adapter_layers.values():
                trainable_params.extend(adapter_layer.parameters())
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ê²½ëŸ‰ íŒŒë¼ë¯¸í„°ë§Œ)
            optimizer = torch.optim.AdamW(trainable_params, lr=self.learning_rate)
            
            # ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµ
            for i in range(0, len(training_data), self.batch_size):
                batch = training_data[i:i+self.batch_size]
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                questions = [data['question'] for data in batch]
                answers = [data['answer'] for data in batch]
                
                # í† í¬ë‚˜ì´ì§•
                question_tokens = self.tokenizer.encode(questions)
                answer_tokens = self.tokenizer.encode(answers)
                
                # íŒ¨ë”© ì²˜ë¦¬
                max_q_len = max(len(tokens) for tokens in question_tokens)
                max_a_len = max(len(tokens) for tokens in answer_tokens)
                
                padded_questions = []
                padded_answers = []
                
                for q_tokens, a_tokens in zip(question_tokens, answer_tokens):
                    # ì§ˆë¬¸ íŒ¨ë”©
                    padded_q = q_tokens + [0] * (max_q_len - len(q_tokens))
                    padded_questions.append(padded_q)
                    
                    # ë‹µë³€ íŒ¨ë”©
                    padded_a = a_tokens + [0] * (max_a_len - len(a_tokens))
                    padded_answers.append(padded_a)
                
                # í…ì„œ ë³€í™˜
                question_tensor = torch.tensor(padded_questions, dtype=torch.long).to(self.device)
                answer_tensor = torch.tensor(padded_answers, dtype=torch.long).to(self.device)
                
                # ìˆœì „íŒŒ
                optimizer.zero_grad()
                
                # ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
                output = self.base_model(question_tensor)
                
                # ì†ì‹¤ ê³„ì‚° (êµì°¨ ì—”íŠ¸ë¡œí”¼)
                loss = F.cross_entropy(
                    output.view(-1, output.size(-1)), 
                    answer_tensor.view(-1),
                    ignore_index=0  # íŒ¨ë”© í† í° ë¬´ì‹œ
                )
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
                
                print(f"ë°°ì¹˜ {i//self.batch_size + 1} ì†ì‹¤: {loss.item():.4f}")
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            self.base_model.eval()
            
            # ê²½ëŸ‰ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
            self._save_finetuned_model()
            
        except Exception as e:
            print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def _save_finetuned_model(self):
        """ê²½ëŸ‰ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥"""
        try:
            # LoRA ê°€ì¤‘ì¹˜ ì €ì¥
            lora_weights = {}
            for name, lora_layer in self.lora_layers.items():
                lora_weights[name] = {
                    'lora_A': lora_layer.lora_A.data.cpu(),
                    'lora_B': lora_layer.lora_B.data.cpu(),
                    'scaling': lora_layer.scaling
                }
            
            # Adapter ê°€ì¤‘ì¹˜ ì €ì¥
            adapter_weights = {}
            for name, adapter_layer in self.adapter_layers.items():
                adapter_weights[name] = {
                    'down_proj': adapter_layer.down_proj.state_dict(),
                    'up_proj': adapter_layer.up_proj.state_dict()
                }
            
            # ì „ì²´ ëª¨ë¸ ì €ì¥
            save_path = 'models/lightweight_finetuned_model.pth'
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            torch.save({
                'base_model_state_dict': self.base_model.state_dict(),
                'lora_weights': lora_weights,
                'adapter_weights': adapter_weights,
                'training_count': self.training_count,
                'last_training_time': self.last_training_time.isoformat() if self.last_training_time else None,
                'vocab_size': self.tokenizer.get_vocab_size(),
                'd_model': self.base_model.d_model,
                'num_layers': self.base_model.num_layers,
                'num_heads': self.base_model.num_heads,
                'd_ff': self.base_model.d_ff
            }, save_path)
            
            print(f"ê²½ëŸ‰ íŒŒì¸íŠœë‹ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_response(self, user_input):
        """ê²½ëŸ‰ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.base_model or not self.tokenizer:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ì…ë ¥ í† í¬ë‚˜ì´ì§•
            input_tokens = self.tokenizer.encode([user_input])
            input_tensor = torch.tensor(input_tokens, dtype=torch.long).to(self.device)
            
            # ì¶”ë¡ 
            with torch.no_grad():
                output = self.base_model(input_tensor)
                
                # ë‹¤ìŒ í† í° ì˜ˆì¸¡
                next_token = torch.argmax(output[0, -1, :])
                
                # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë””ì½”ë”© í•„ìš”)
                response_tokens = [next_token.item()]
                
                # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                response = self.tokenizer.decode([response_tokens])
                
                return response if response else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def get_stats(self):
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return {
            'training_count': self.training_count,
            'buffer_size': len(self.training_buffer),
            'lora_layers': len(self.lora_layers),
            'adapter_layers': len(self.adapter_layers),
            'last_training_time': self.last_training_time.isoformat() if self.last_training_time else None,
            'is_training': self.is_training
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
lightweight_finetuner = None

def initialize_lightweight_finetuner():
    """ê²½ëŸ‰ íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”"""
    global lightweight_finetuner
    lightweight_finetuner = LightweightFineTuner()
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    if lightweight_finetuner.load_base_model():
        # LoRA ì¶”ê°€
        lightweight_finetuner.add_lora_to_model()
        # Adapter ì¶”ê°€
        lightweight_finetuner.add_adapters_to_model()
        return True
    else:
        return False

def get_lightweight_finetuner():
    """ê²½ëŸ‰ íŒŒì¸íŠœë„ˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return lightweight_finetuner 